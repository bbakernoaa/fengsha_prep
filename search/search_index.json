{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"fengsha_prep","text":"<p>Tools to retrieve and process inputs needed for the NOAA fengsha dust emission model.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install the package, clone the repository and install the dependencies:</p> <pre><code>git clone https://github.com/bbakernoaa/fengsha_prep.git\ncd fengsha_prep\npip install .\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<p>The <code>fengsha_prep</code> package requires a configuration file located at <code>src/fengsha_prep/data_downloaders/config.toml</code>. This file contains the URLs for the BNU soil dataset. A sample <code>config.toml</code> file is provided with placeholder URLs. You will need to replace these with the actual download links.</p> <pre><code>[bnu_data]\n# Please replace these placeholder URLs with the actual download links.\nsand_urls = [\n  \"http://example.com/path/to/sand/data1.nc\",\n  \"http://example.com/path/to/sand/data2.nc\",\n]\nsilt_urls = [\n  \"http://example.com/path/to/silt/data1.nc\",\n  \"http://example.com/path/to/silt/data2.nc\",\n]\nclay_urls = [\n  \"http://example.com/path/to/clay/data1.nc\",\n  \"http://example.com/path/to/clay/data2.nc\",\n]\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#soilgrids","title":"SoilGrids","text":"<p>To retrieve data from SoilGrids, use the <code>get_soilgrids_data</code> function. The data will be saved as a compressed NetCDF file.</p> <pre><code>from fengsha_prep.soilgrids import get_soilgrids_data\n\ndata = get_soilgrids_data(\n    service_id='sand',\n    coverage_id='sand_0-5cm_mean',\n    west=-10,\n    south=-10,\n    east=10,\n    north=10,\n    crs='urn:ogc:def:crs:EPSG::4326',\n    output_path='sand_0-5cm_mean.nc'\n)\n</code></pre>"},{"location":"#bnu","title":"BNU","text":"<p>To retrieve data from the BNU soil dataset, use the <code>get_bnu_data</code> function. The <code>data_type</code> argument should correspond to an entry in the <code>config.toml</code> file (e.g., 'sand', 'silt', 'clay').</p> <pre><code>from fengsha_prep.bnu import get_bnu_data\n\ndownloaded_files = get_bnu_data('sand', output_dir='bnu_sand_data')\n</code></pre>"},{"location":"#regridding","title":"Regridding","text":"<p>To regrid a dataset from a MODIS sinusoidal grid to a rectilinear Gaussian grid, use the <code>regrid_modis_to_rectilinear</code> function.</p> <pre><code>import xarray as xr\nimport numpy as np\nfrom fengsha_prep.regrid import regrid_modis_to_rectilinear\n\n# Create a dummy sinusoidal dataset\nds_sinu = xr.Dataset({\n    'foo': (('y', 'x'), np.random.rand(10, 20)),\n    'lat': (('y', 'x'), np.random.uniform(20, 50, size=(10, 20))),\n    'lon': (('y', 'x'), np.random.uniform(-120, -70, size=(10, 20))),\n})\n\n# Define the output grid\nlon_min, lon_max, d_lon = -110, -80, 1.0\nlat_min, lat_max, d_lat = 30, 45, 1.0\n\n# Regrid the dataset\nds_regridded = regrid_modis_to_rectilinear(\n    ds_sinu, 'foo', lon_min, lon_max, d_lon, lat_min, lat_max, d_lat\n)\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.</p> <p>Please make sure to update tests as appropriate.</p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This page provides an auto-generated API reference for the <code>fengsha_prep</code> package.</p>"},{"location":"user_guide/data_downloaders/","title":"Data Downloaders","text":"<p>This guide provides detailed information on how to use the data downloaders available in <code>fengsha_prep</code>.</p>"},{"location":"user_guide/data_downloaders/#bnu-downloader","title":"BNU Downloader","text":"<p>The BNU downloader retrieves soil data from the BNU dataset. The available data types are <code>sand</code>, <code>silt</code>, and <code>clay</code>.</p>"},{"location":"user_guide/data_downloaders/#configuration","title":"Configuration","text":"<p>Before using the downloader, you need to configure the download URLs in the <code>src/fengsha_prep/data_downloaders/config.toml</code> file.</p> <pre><code>[bnu_data]\nsand_urls = [\"http://example.com/sand.nc\"]\nsilt_urls = [\"http://example.com/silt.nc\"]\nclay_urls = [\"http://example.com/clay.nc\"]\n</code></pre>"},{"location":"user_guide/data_downloaders/#usage","title":"Usage","text":"<p>Here's how to use the <code>get_bnu_data</code> function:</p> <pre><code>from fengsha_prep.data_downloaders import bnu\n\nbnu.get_bnu_data(data_type=\"sand\", output_dir=\"bnu_data\")\n</code></pre>"},{"location":"user_guide/data_downloaders/#soilgrids-downloader","title":"SoilGrids Downloader","text":"<p>The SoilGrids downloader fetches soil data from the SoilGrids service.</p>"},{"location":"user_guide/data_downloaders/#usage_1","title":"Usage","text":"<p>The <code>get_soilgrids_data</code> function allows you to specify the service, coverage, and geographic bounding box.</p> <pre><code>from fengsha_prep.data_downloaders import soilgrids\n\nsoilgrids.get_soilgrids_data(\n    service_id=\"sand\",\n    coverage_id=\"sand_0-5cm_mean\",\n    west=-10,\n    south=-10,\n    east=10,\n    north=10,\n    output_path=\"soilgrids_sand.nc\",\n)\n</code></pre>"},{"location":"user_guide/pipelines/","title":"Pipelines","text":"<p>This guide explains the data processing pipelines available in <code>fengsha_prep</code>.</p>"},{"location":"user_guide/pipelines/#drag-partition-pipeline","title":"Drag Partition Pipeline","text":"<p>The drag partition pipeline implements a hybrid model to estimate the surface friction velocity (<code>us*</code>). This is a key parameter in determining how much wind energy is transferred to the ground, which is critical for predicting dust emissions.</p> <p>The pipeline works by partitioning the drag forces between: - Bare Soil: Using the model from Chappell &amp; Webb (2016). - Green Vegetation: Based on the work of Leung et al. (2023). - Brown (Non-Photosynthetic) Vegetation: Modeled according to Guerschman et al. (2009).</p>"},{"location":"user_guide/pipelines/#usage","title":"Usage","text":"<p>The main function, <code>process_hybrid_drag</code>, automates the entire workflow. It fetches the required MODIS Albedo (MCD43C3) and Leaf Area Index (MCD15A2H) data from NASA's Earthdata cloud, and then calculates the surface friction velocity.</p> <pre><code>from fengsha_prep.pipelines.drag_partition import core\n\n# Define the time period for the analysis\nstart_date = \"2024-03-01\"\nend_date = \"2024-03-07\"\n\n# Provide the 10-meter wind speed (can be a constant or an xarray DataArray)\nwind_speed = 7.5  # in m/s\n\n# Run the pipeline\nsurface_friction_velocity = core.process_hybrid_drag(\n    start_date=start_date,\n    end_date=end_date,\n    u10_wind=wind_speed\n)\n\nprint(surface_friction_velocity)\n</code></pre>"},{"location":"user_guide/pipelines/#dust-scan-pipeline","title":"Dust Scan Pipeline","text":"<p>The dust scan pipeline is an asynchronous tool designed to detect and cluster dust plumes from satellite imagery over a specified time period. It can process data from GOES satellites (via AWS S3) or other satellites with local data.</p> <p>The pipeline performs the following steps: 1.  Loads Satellite Data: Asynchronously loads satellite scenes, handling both S3 and local file access. 2.  Detects Dust: Applies a physical dust detection algorithm based on brightness temperature differences between infrared channels. 3.  Clusters Events: Uses the DBSCAN clustering algorithm to group dusty pixels into distinct plumes, calculating properties like the centroid and area for each event.</p>"},{"location":"user_guide/pipelines/#usage_1","title":"Usage","text":"<p>The pipeline is typically run from the command line. You need to provide the satellite ID, start and end times, and an output file path.</p> <pre><code>python -m src.fengsha_prep.pipelines.dust_scan.core \\\n    --sat goes16 \\\n    --start 2024-04-01T18:00 \\\n    --end 2024-04-01T22:00 \\\n    --output dust_events_report.csv\n</code></pre> <p>This will generate a CSV file (<code>dust_events_report.csv</code>) listing all the dust plumes detected in the specified four-hour window.</p>"},{"location":"user_guide/pipelines/#uthresh-pipeline-gde-piml","title":"Uthresh Pipeline (GDE-PIML)","text":"<p>The <code>uthresh</code> pipeline is a comprehensive, end-to-end suite for modeling global dust emissions, referred to as the Global Dust Emission Physics-Informed Machine Learning (GDE-PIML) Suite. It integrates data retrieval, physics-based calculations, and a machine learning model to produce global dust flux maps.</p> <p>The pipeline is divided into four main stages:</p> <ol> <li>Data Retrieval: Fetches meteorological data (from UFS Replay on S3) and soil properties (from SoilGrids) for a given location.</li> <li>Physics Engine: Calculates two key physical parameters:<ul> <li>The hybrid drag partition ratio, which determines how wind stress is partitioned between bare soil and vegetation.</li> <li>The moisture inhibition factor, which models how soil moisture suppresses dust emission.</li> </ul> </li> <li>PIML Model: Uses a trained XGBoost model to predict the threshold friction velocity required to initiate dust emission, based on the physical parameters and soil properties.</li> <li>Flux Projection: Combines the predicted threshold velocity with meteorological data to calculate the vertical dust flux using the Marticorena-Bergametti (1995) flux equation.</li> </ol> <p>Due to its complexity and focus on large-scale global modeling, the <code>uthresh</code> pipeline is not typically used for simple, small-scale examples. It is designed to be integrated into larger climate and weather modeling workflows.</p>"}]}